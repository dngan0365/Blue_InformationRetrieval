{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79aaddff",
   "metadata": {},
   "source": [
    "# Indexing voi mo hinh xac suat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "770312b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from whoosh.index import create_in\n",
    "from whoosh.fields import *\n",
    "from whoosh.analysis import StandardAnalyzer\n",
    "from whoosh import qparser\n",
    "from whoosh import scoring\n",
    "import whoosh.index as index\n",
    "\n",
    "import pytrec_eval\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530758b",
   "metadata": {},
   "source": [
    "**1. Tien xu ly du lieu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4469df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import py_vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301797d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tôi yêu xử_lý ngôn_ngữ_tự_nhiên .']\n"
     ]
    }
   ],
   "source": [
    "def load_vncorenlp(model_dir):\n",
    "    import os, py_vncorenlp\n",
    "    \n",
    "    jar_path = os.path.join(model_dir, \"VnCoreNLP-1.2.jar\")\n",
    "    if not os.path.exists(jar_path):\n",
    "        raise FileNotFoundError(f\"❌ Missing {jar_path}\")\n",
    "    \n",
    "    if not os.path.exists(os.path.join(model_dir, \"models\")):\n",
    "        raise FileNotFoundError(f\"❌ Missing model folder at {model_dir}/models\")\n",
    "    \n",
    "    return py_vncorenlp.VnCoreNLP(\n",
    "        annotators=[\"wseg\", \"pos\", \"ner\", \"parse\"],\n",
    "        save_dir=model_dir,\n",
    "        max_heap_size='-Xmx2g'\n",
    "    )\n",
    "\n",
    "model = load_vncorenlp(r\"C:/Users/mt200/OneDrive/Desktop/AI/InformationRetrieval/Project_InformationRetrieval/Reference/VnCoreNLP-master/\")\n",
    "print(model.word_segment(\"tôi yêu xử lý ngôn ngữ tự nhiên.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65441947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bình ba là một đảo nhỏ diện_tích trên 3 km² thuộc xã cam bình , thành_phố cam_ranh , tỉnh khánh_hoà , việt_nam .']\n"
     ]
    }
   ],
   "source": [
    "print(model.word_segment(\"bình ba là một đảo nhỏ diện tích trên 3 km² thuộc xã cam bình, thành phố cam ranh, tỉnh khánh hòa, việt nam.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b856f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = ['.', ',', ':', '`', '\"', \"'\", '!', '?', \"``\", \"''\"]\n",
    "stoplist = set()\n",
    "with open(r\"C:\\Users\\mt200\\OneDrive\\Desktop\\AI\\InformationRetrieval\\Project_InformationRetrieval\\3.Indexing\\stopword\\vietnamese-stopwords-dash.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        stoplist.add(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2555f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tok, punctlist=puncts, stopwords=stoplist):\n",
    "  tok = tok.lower()\n",
    "  if tok.isdigit():\n",
    "    return None\n",
    "  if tok.isnumeric():\n",
    "    return None\n",
    "  if tok in punctlist:\n",
    "    return None\n",
    "  if tok in stopwords:\n",
    "    return None\n",
    "  return tok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba18d22",
   "metadata": {},
   "source": [
    "**2. Lap chi muc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf5d5f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexing(src, idx=\"ind\"):\n",
    "    if not src.endswith('/'):\n",
    "        src += '/'\n",
    "\n",
    "    # Ensure index directory exists\n",
    "    os.makedirs(idx, exist_ok=True)\n",
    "\n",
    "    schema = Schema(docid=STORED(), content=TEXT(stored=True, analyzer=StandardAnalyzer()))\n",
    "    ix = create_in(idx, schema)\n",
    "    writer = ix.writer()\n",
    "\n",
    "    files = os.listdir(src)\n",
    "    for f in files:\n",
    "        file_path = os.path.join(src, f)\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        encodings_to_try = ['utf-8', 'utf-16', 'cp1252']\n",
    "        content = None\n",
    "        for enc in encodings_to_try:\n",
    "            try:\n",
    "                with open(file_path, encoding=enc) as r:\n",
    "                    content = r.read()\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "        if content is None:\n",
    "            print(f\"⚠️ Skipped file {f}: could not decode with {encodings_to_try}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            terms = []\n",
    "            for sent in sent_tokenize(content.strip()):\n",
    "                temp_sent_dash = model.word_segment(sent)\n",
    "\n",
    "                # ✅ FIX: join list of tokens into one string\n",
    "                if isinstance(temp_sent_dash, list):\n",
    "                    temp_sent_dash = \" \".join(temp_sent_dash)\n",
    "\n",
    "                for tok in word_tokenize(temp_sent_dash):\n",
    "                    tok = preprocess(tok)\n",
    "                    if tok:\n",
    "                        terms.append(tok)\n",
    "\n",
    "            cont = \" \".join(terms)\n",
    "            writer.add_document(docid=f.split(\".\")[0], content=cont)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipped file {f}: {e}\")\n",
    "\n",
    "    writer.commit()\n",
    "    print(f\"✅ Indexing completed. Index stored in: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5f03d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipped file Chinh_phuc_thac_Lieng_Nung_-_ve_ep_sieu_thuc_cua_Tay_Nguyen.txt: JVM exception occurred: java.lang.ArrayIndexOutOfBoundsException\n",
      "java.lang.ArrayIndexOutOfBoundsException\n",
      "✅ Indexing completed. Index stored in: ind\n"
     ]
    }
   ],
   "source": [
    "indexing(\"C:/Users/mt200/OneDrive/Desktop/AI/InformationRetrieval/Project_InformationRetrieval/1.CollectingDocuments/data_clean\", \"ind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732a0b1",
   "metadata": {},
   "source": [
    "**3. Kiem tra mot so truy van**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf6de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "from whoosh.index import open_dir\n",
    "from whoosh import scoring  # ⚡ cần import thêm\n",
    "\n",
    "def search_query(idx=\"ind\", query_str=\"\"):\n",
    "    \"\"\"\n",
    "    Thực hiện truy vấn trên chỉ mục Whoosh bằng mô hình xác suất BM25.\n",
    "    - idx: đường dẫn đến thư mục chỉ mục\n",
    "    - query_str: chuỗi truy vấn (từ khóa người dùng)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Mở thư mục chỉ mục\n",
    "        ix = open_dir(idx)\n",
    "\n",
    "        # Tạo parser cho trường 'content'\n",
    "        qp = QueryParser(\"content\", schema=ix.schema)\n",
    "        q = qp.parse(query_str)\n",
    "\n",
    "        # ⚡ Sử dụng mô hình BM25 thay vì TF-IDF\n",
    "        with ix.searcher(weighting=scoring.BM25F()) as searcher:\n",
    "            results = searcher.search(q, limit=10)\n",
    "\n",
    "            if not results:\n",
    "                print(\"❌ Không tìm thấy tài liệu phù hợp.\")\n",
    "                return\n",
    "\n",
    "            print(f\"✅ Tìm thấy {len(results)} tài liệu liên quan (BM25):\\n\")\n",
    "            for rank, hit in enumerate(results, start=1):\n",
    "                print(f\"{rank}. DocID: {hit['docid']}\")\n",
    "                snippet = hit['content'][:150].replace('\\n', ' ')\n",
    "                print(f\"   Trích đoạn: {snippet}...\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Lỗi khi truy vấn: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bafacb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tìm thấy 77 tài liệu liên quan (BM25):\n",
      "\n",
      "1. DocID: Hon_Tam__Wikipedia_tieng_Viet\n",
      "   Trích đoạn: hòn tằm ( gọi thuỷ kim_sơn ) hòn đảo rộng ha nằm vịnh nha_trang nằm km đông nam thành_phố hòn đảo bãi biển đẹp nha_trang thu_hút du_lịch tổng_quan đảo...\n",
      "\n",
      "2. DocID: Cam_nang_du_lich_Kien_Giang\n",
      "   Trích đoạn: du_lịch cẩm_nang kiên_giang trở_lại du_lịch hướng di_chuyển lưu_trú tham_quan ẩm_thực lưu_ý kiên_giang đồng_bằng sông_cửu_long tỉnh diện_tích tây_nam_...\n",
      "\n",
      "3. DocID: Cam_nang_du_lich_Nam_Du\n",
      "   Trích đoạn: du_lịch cẩm_nang nam du trở_lại du_lịch hướng di_chuyển lưu_trú tham_quan ẩm_thực lưu_ý nam du hai xã an sơn nam du huyện kiên_hải diện_tích ha hòn đả...\n",
      "\n",
      "4. DocID: Cam_nang_du_lich_Co_To\n",
      "   Trích đoạn: du_lịch cẩm_nang cô_tô trở_lại du_lịch hướng cô_tô mùa đẹp di_chuyển ăn_uống homestay nhà_nghỉ lưu_ý cô_tô huyện đảo đông tỉnh quảng_ninh đất_liền km ...\n",
      "\n",
      "5. DocID: Cua_Lo_(thi_xa)__Wikipedia_tieng_Viet\n",
      "   Trích đoạn: x t s cửa_lò thị_xã cũ nằm đông nam tỉnh nghệ_an thị_xã cửa_lò sáp_nhập thành_phố vinh hành_chính cửa_lò tái_thiết lập đợt sáp_nhập tỉnh_thành việt_na...\n",
      "\n",
      "6. DocID: Cam_nang_du_lich_Khanh_Hoa\n",
      "   Trích đoạn: du_lịch cẩm_nang khánh_hoà trở_lại du_lịch hướng di_chuyển lưu_trú tham_quan ẩm_thực đặc_sản khánh_hoà tỉnh ven biển nam_trung_bộ thủ_phủ thành_phố nh...\n",
      "\n",
      "7. DocID: Vinh_Ha_Long__Wikipedia_tieng_Viet\n",
      "   Trích đoạn: vịnh hạ_long vịnh bờ tây vịnh bắc_bộ khu_vực biển đông bắc việt_nam bao_gồm vùng_biển đảo thành_phố hạ_long tỉnh quảng_ninh trung_tâm khu_vực rộng_lớn...\n",
      "\n",
      "8. DocID: Cam_nang_du_lich_Quang_Tri\n",
      "   Trích đoạn: du_lịch cẩm_nang quảng_trị trở_lại du_lịch hướng di_chuyển lưu_trú tham_quan ẩm_thực lưu_ý quảng_trị nằm ven biển miền trung trung_tâm hành_chính phườ...\n",
      "\n",
      "9. DocID: Cam_nang_du_lich_Quang_Ngai\n",
      "   Trích đoạn: du_lịch cẩm_nang quảng_ngãi trở_lại du_lịch hướng mùa đẹp di_chuyển lưu_trú lưu_ý tỉnh quảng_ngãi nằm miền nam_trung_bộ giáp tỉnh quảng_nam bắc bình_đ...\n",
      "\n",
      "10. DocID: Kham_pha_ve_ep_ao_xanh_Cu_Lao_Cham\n",
      "   Trích đoạn: giới_thiệu chức_năng nhiệm_vụ cơ_cấu tổ_chức lịch_sử ngành hình_ảnh tư_liệu văn_bản - thủ_tục tra_cứu văn_bản thủ_tục hành_chính hướng_dẫn nộp phí lệ_...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_query(\"ind\", \"biển đảo Việt Nam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e5deb",
   "metadata": {},
   "source": [
    "# Danh gia mo hinh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3303bc9",
   "metadata": {},
   "source": [
    "***2) Xử lý truy vấn***\n",
    "- Chuẩn bị tập Ground Truth\n",
    "- Chuẩn bị tập câu truy vấn.\n",
    "- Truy vấn chỉ mục với mô hình xác suất, trọng số term tính theo TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a8b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGroundTruth(src):\n",
    "  if src[-1] != '/':\n",
    "    src += '/'\n",
    "\n",
    "  GT = {}\n",
    "  for f in os.listdir(src):\n",
    "    r = open(src + f)\n",
    "    rel = {}\n",
    "    for s in r:\n",
    "      s = s.strip()\n",
    "      sp = s.split(\"\\t\")\n",
    "      if len(sp) < 2:\n",
    "        continue\n",
    "      did = sp[0].split(\" \")[1]\n",
    "      rel[did] = int(sp[1])\n",
    "    GT[f.split(\".\")[0]] = rel\n",
    "    r.close()\n",
    "  return GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GroundTruth = readGroundTruth(\"Cranfield/RES\")\n",
    "print(GroundTruth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aec5141",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readQuery(src):\n",
    "  qry = {}\n",
    "  r = open(src)\n",
    "  for s in r:\n",
    "    s = s.strip()\n",
    "    ps = s.split(\"\\t\")\n",
    "    terms = []\n",
    "    for sent in sent_tokenize(ps[1]):\n",
    "      for tok in word_tokenize(sent):\n",
    "        tok = preprocess(tok)\n",
    "        if tok != None:\n",
    "          terms.append(tok)\n",
    "    qry[ps[0]] = \" \".join(terms)\n",
    "  return qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d14663",
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries = readQuery(\"Cranfield/query.txt\")\n",
    "print(Queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQueries(ind, qry):\n",
    "\n",
    "  idx = index.open_dir(ind)\n",
    "  searcher = idx.searcher(weighting=scoring.BM25F())\n",
    "  parser = qparser.QueryParser(\"content\", idx.schema, group=qparser.OrGroup)\n",
    "\n",
    "  RET = {}\n",
    "  for key in qry:\n",
    "    query = parser.parse(qry[key])\n",
    "    results = searcher.search(query, limit=None)\n",
    "    rel = {}\n",
    "    for i in range(len(results)):\n",
    "      rel[results[i][\"docid\"]] = results[i].score\n",
    "    RET[key] = rel\n",
    "  return RET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b46cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RunResults = processQueries(\"ind\", Queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f6763",
   "metadata": {},
   "source": [
    "***3) Tính chỉ số***\n",
    "- Sử dụng đối tượng thuộc lớp RelevanceEvaluator từ package pytrec_eval.\n",
    "- Tính chỉ số MAP (\"intAP\") và MAPint (\"11pt_avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pytrec_eval.supported_measures)\n",
    "eval = pytrec_eval.RelevanceEvaluator(GroundTruth, [\"infAP\", \"11pt_avg\"])\n",
    "Results = eval.evaluate(RunResults)\n",
    "\n",
    "MAP = 0\n",
    "MAP11PT = 0\n",
    "\n",
    "for key in Results:\n",
    "  value = Results[key][\"infAP\"]\n",
    "  if not math.isnan(value):\n",
    "    MAP += value\n",
    "  value = Results[key][\"11pt_avg\"]\n",
    "  if not math.isnan(value):\n",
    "    MAP11PT += value\n",
    "\n",
    "MAP /= len(Results)\n",
    "MAP11PT /= len(Results)\n",
    "\n",
    "print(MAP, MAP11PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c8671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
