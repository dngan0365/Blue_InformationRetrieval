{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46cd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datasketch import MinHash\n",
    "from itertools import combinations\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08984808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Cáº¥u hÃ¬nh ======\n",
    "FOLDER_PATH = \"../1.CollectingDocuments/data_clean\"  # ThÆ° má»¥c chá»©a cÃ¡c file .txt\n",
    "NGRAMS = [2, 3]       # DÃ¹ng bigram vÃ  trigram\n",
    "MINHASH_SIZE = 128    # Sá»‘ lÆ°á»£ng hash (kÃ­ch thÆ°á»›c vector minhash)\n",
    "SIM_THRESHOLD = 0.5   # NgÆ°á»¡ng tÆ°Æ¡ng tá»± Ä‘á»ƒ xem lÃ  trÃ¹ng\n",
    "ENCODING = \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a55be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 1. LÃ m sáº¡ch text ======\n",
    "def clean_text(text):\n",
    "    \"\"\"Chuyá»ƒn chá»¯ thÆ°á»ng, bá» kÃ½ tá»± Ä‘áº·c biá»‡t, giá»¯ láº¡i chá»¯ & sá»‘.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82e5283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 2. Sinh n-gram ======\n",
    "def get_ngrams(text, n):\n",
    "    \"\"\"Tráº£ vá» danh sÃ¡ch cÃ¡c n-gram (chuá»—i n tá»« liÃªn tiáº¿p).\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return [\" \".join(gram) for gram in ngrams(tokens, n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425c2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 3. Táº¡o MinHash ======\n",
    "def create_minhash(ngrams_set, num_perm=MINHASH_SIZE):\n",
    "    \"\"\"Sinh chá»¯ kÃ½ MinHash tá»« táº­p n-gram.\"\"\"\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for ng in ngrams_set:\n",
    "        m.update(ng.encode(\"utf8\"))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48928b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 4. Äá»c cÃ¡c file trong thÆ° má»¥c ======\n",
    "def load_documents(folder):\n",
    "    docs = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            path = os.path.join(folder, filename)\n",
    "            with open(path, \"r\", encoding=ENCODING) as f:\n",
    "                text = f.read()\n",
    "                docs[filename] = text\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23ce68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 5. Tiá»n xá»­ lÃ½ + sinh MinHash cho tá»«ng doc ======\n",
    "def build_minhashes(docs):\n",
    "    minhashes = {}\n",
    "    for name, text in tqdm(docs.items(), desc=\"Táº¡o MinHash\"):\n",
    "        text = clean_text(text)\n",
    "        ngrams_all = set()\n",
    "        for n in NGRAMS:\n",
    "            ngrams_all |= set(get_ngrams(text, n))\n",
    "        minhashes[name] = create_minhash(ngrams_all)\n",
    "    return minhashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5c1fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 6. So sÃ¡nh cáº·p tÃ i liá»‡u ======\n",
    "def compute_similarities(minhashes, threshold=SIM_THRESHOLD):\n",
    "    similar_pairs = []\n",
    "    for (doc1, m1), (doc2, m2) in tqdm(list(combinations(minhashes.items(), 2)), desc=\"So sÃ¡nh\"):\n",
    "        sim = m1.jaccard(m2)\n",
    "        if sim >= threshold:\n",
    "            similar_pairs.append((doc1, doc2, round(sim, 3)))\n",
    "    return similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20d82713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 7. Ghi káº¿t quáº£ ra CSV ======\n",
    "def save_results(pairs, output_path=\"similar_docs.csv\"):\n",
    "    import csv\n",
    "    with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Document 1\", \"Document 2\", \"Similarity\"])\n",
    "        for d1, d2, s in pairs:\n",
    "            writer.writerow([d1, d2, s])\n",
    "    print(f\"âœ… Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d00dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Äang Ä‘á»c file...\n",
      "ğŸ“„ Tá»•ng sá»‘ tÃ i liá»‡u: 407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Táº¡o MinHash: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 407/407 [02:42<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "So sÃ¡nh: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82621/82621 [00:01<00:00, 48146.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§­ CÃ¡c cáº·p tÃ i liá»‡u trÃ¹ng hoáº·c giá»‘ng nhau:\n",
      " - Cam_nang_du_lich_Ky_Co.txt <-> Cam_nang_du_lich_Lam_ong.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 1.0\n",
      "âœ… Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o similar_docs.csv\n"
     ]
    }
   ],
   "source": [
    "# ====== 8. Cháº¡y toÃ n bá»™ pipeline ======\n",
    "print(\"ğŸ“‚ Äang Ä‘á»c file...\")\n",
    "docs = load_documents(FOLDER_PATH)\n",
    "\n",
    "print(f\"ğŸ“„ Tá»•ng sá»‘ tÃ i liá»‡u: {len(docs)}\")\n",
    "minhashes = build_minhashes(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4bc1bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Äang tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»±...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "So sÃ¡nh: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 82621/82621 [00:01<00:00, 52337.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§­ CÃ¡c cáº·p tÃ i liá»‡u trÃ¹ng hoáº·c giá»‘ng nhau:\n",
      " - Cam_nang_du_lich_Hung_Yen.txt <-> Cam_nang_du_lich_Ninh_Chu.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.648\n",
      " - Cam_nang_du_lich_Ky_Co.txt <-> Cam_nang_du_lich_Lam_ong.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 1.0\n",
      " - Cam_nang_du_lich_Ninh_Chu.txt <-> Cam_nang_du_lich_Thai_Binh.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.523\n",
      " - Cam_nang_du_lich_Quang_Binh.txt <-> Cam_nang_du_lich_Quang_Tri.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.5\n",
      " - Chiem_nguong_ve_ep_hiem_co_o_Thac_Mo_Binh_Phuoc_-_HoaBinhTourist.txt <-> Thac_Mo_Binh_Phuoc_Kham_pha_ve_ep_hiem_co_kho_tim.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.539\n",
      " - Hang_En__Wikipedia_tieng_Viet.txt <-> ong_Phong_Nha__Wikipedia_tieng_Viet.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.539\n",
      " - Khu_Du_Lich_Buu_Long,_Toa_o_Vui_Choi_Cuc_Hot_Gan_Sai_Gon_-_Klook_Blog.txt <-> Kinh_Nghiem_i_ia_ao_Cu_Chi,_Di_Tich_Noi_Tieng_O_Sai_Gon_-_Klook_Blog.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.609\n",
      " - Thac_amb'ri__Wikipedia_tieng_Viet.txt <-> Thac_Pongour__Wikipedia_tieng_Viet.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.602\n",
      " - Thac_amb'ri__Wikipedia_tieng_Viet.txt <-> Thac_Prenn__Wikipedia_tieng_Viet.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.57\n",
      " - Thac_Pongour__Wikipedia_tieng_Viet.txt <-> Thac_Prenn__Wikipedia_tieng_Viet.txt: Ä‘á»™ tÆ°Æ¡ng tá»± = 0.555\n",
      "âœ… Káº¿t quáº£ Ä‘Ã£ lÆ°u vÃ o similar_docs.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Äang tÃ­nh Ä‘á»™ tÆ°Æ¡ng tá»±...\")\n",
    "similar_pairs = compute_similarities(minhashes)\n",
    "\n",
    "if similar_pairs:\n",
    "    print(\"\\nğŸ§­ CÃ¡c cáº·p tÃ i liá»‡u trÃ¹ng hoáº·c giá»‘ng nhau:\")\n",
    "    for d1, d2, s in similar_pairs:\n",
    "        print(f\" - {d1} <-> {d2}: Ä‘á»™ tÆ°Æ¡ng tá»± = {s}\")\n",
    "    save_results(similar_pairs)\n",
    "else:\n",
    "    print(\"âŒ KhÃ´ng cÃ³ cáº·p tÃ i liá»‡u nÃ o giá»‘ng nhau vÆ°á»£t ngÆ°á»¡ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1087039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” PhÃ¡t hiá»‡n 1 cáº·p cÃ³ Similarity > 0.8\n",
      "ğŸ“‚ Sáº½ xÃ³a 1 file trÃ¹ng láº·p:\n",
      "  - Cam_nang_du_lich_Lam_ong.txt\n",
      "â Há»§y thao tÃ¡c.\n",
      "\n",
      "âœ… ÄÃ£ xÃ³a thÃ nh cÃ´ng 1/1 file cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng > 0.8\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ==== Cáº¤U HÃŒNH ====\n",
    "csv_path = \"similar_docs.csv\"  # File CSV chá»©a cÃ¡c cáº·p vÃ  Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng\n",
    "folder_path = \"../1.CollectingDocuments/data_clean\"  # ThÆ° má»¥c chá»©a cÃ¡c file .txt\n",
    "threshold = 0.8  # NgÆ°á»¡ng tÆ°Æ¡ng Ä‘á»“ng\n",
    "\n",
    "# ==== Äá»ŒC FILE CSV ====\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Lá»c cÃ¡c cáº·p cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao hÆ¡n ngÆ°á»¡ng\n",
    "high_sim = df[df[\"Similarity\"] > threshold]\n",
    "\n",
    "print(f\"ğŸ” PhÃ¡t hiá»‡n {len(high_sim)} cáº·p cÃ³ Similarity > {threshold}\")\n",
    "\n",
    "# ==== XÃC Äá»ŠNH DANH SÃCH FILE Cáº¦N XÃ“A ====\n",
    "files_to_delete = set(high_sim[\"Document 2\"].tolist())  # chá»‰ xÃ³a file thá»© 2 trong má»—i cáº·p\n",
    "\n",
    "print(f\"ğŸ“‚ Sáº½ xÃ³a {len(files_to_delete)} file trÃ¹ng láº·p:\")\n",
    "\n",
    "for f in files_to_delete:\n",
    "    print(\"  -\", f)\n",
    "\n",
    "# ==== XÃC NHáº¬N TRÆ¯á»šC KHI XÃ“A ====\n",
    "confirm = input(\"\\nâ“Báº¡n cÃ³ cháº¯c cháº¯n muá»‘n xÃ³a cÃ¡c file nÃ y khÃ´ng? (y/n): \").lower()\n",
    "if confirm != \"y\":\n",
    "    print(\"â Há»§y thao tÃ¡c.\")\n",
    "    exit()\n",
    "\n",
    "# ==== XÃ“A FILE ====\n",
    "deleted = 0\n",
    "for f in files_to_delete:\n",
    "    file_path = os.path.join(folder_path, f)\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            deleted += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ KhÃ´ng thá»ƒ xÃ³a {f}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… ÄÃ£ xÃ³a thÃ nh cÃ´ng {deleted}/{len(files_to_delete)} file cÃ³ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng > {threshold}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
